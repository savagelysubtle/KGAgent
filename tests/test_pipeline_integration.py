"""Integration tests for the complete Docling-to-ChromaDB pipeline with embeddings."""

import json
import tempfile
import shutil
from pathlib import Path
from unittest.mock import patch, MagicMock

import pytest

from src.kg_agent.pipeline.manager import PipelineManager
from src.kg_agent.services.embedder import EmbedderService
from src.kg_agent.services.vector_store import VectorStoreService
from src.kg_agent.models.chunk import Chunk, ChunkBatch


@pytest.mark.asyncio
class TestPipelineIntegration:
    """Test the complete pipeline from file processing to vector storage."""

    def setup_method(self):
        """Setup test environment."""
        # Create temporary directories for testing
        self.temp_dir = Path(tempfile.mkdtemp())
        self.test_data_dir = self.temp_dir / "test_data"
        self.test_data_dir.mkdir()

        # Create test HTML content
        self.test_html_content = """
        <!DOCTYPE html>
        <html>
        <head><title>Test Document</title></head>
        <body>
            <h1>Knowledge Graph Pipeline Test</h1>
            <p>This is a test document to verify the complete pipeline integration.</p>
            <p>The pipeline should parse this HTML, chunk the content, generate embeddings using the local model, and store them in ChromaDB.</p>
            <h2>Technical Components</h2>
            <p>The system integrates multiple technologies:</p>
            <ul>
                <li>Docling for document parsing</li>
                <li>LangChain for text chunking</li>
                <li>HuggingFace embeddings for vectorization</li>
                <li>ChromaDB for vector storage</li>
            </ul>
        </body>
        </html>
        """

    def teardown_method(self):
        """Cleanup test environment."""
        # Clean up temporary directory - use ignore_errors on Windows due to file locks
        if self.temp_dir.exists():
            try:
                shutil.rmtree(self.temp_dir, ignore_errors=True)
            except Exception:
                pass  # Ignore cleanup errors on Windows

    @pytest.mark.asyncio
    async def test_complete_pipeline_with_embeddings(self):
        """Test the complete pipeline flow including embeddings."""
        # Create test file
        test_file = self.test_data_dir / "test_document.html"
        test_file.write_text(self.test_html_content)

        # Initialize pipeline manager
        pipeline = PipelineManager()

        # Run file pipeline
        result = await pipeline.run_file_pipeline([str(test_file)])

        # Verify pipeline completed successfully
        assert result["status"] == "success"
        assert result["metrics"]["files_submitted"] == 1
        assert result["metrics"]["parsed_successfully"] == 1
        assert result["metrics"]["chunked_successfully"] == 1
        assert result["metrics"]["vectors_stored"] >= 1

        # Verify artifacts were created
        assert len(result["artifacts"]["parsed_files"]) == 1
        assert len(result["artifacts"]["chunk_files"]) == 1

        # Verify files exist on disk
        for parsed_file in result["artifacts"]["parsed_files"]:
            assert Path(parsed_file).exists()

        for chunk_file in result["artifacts"]["chunk_files"]:
            assert Path(chunk_file).exists()

            # Load and verify chunk file structure (embeddings are added in-memory during processing)
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
                chunk_batch = ChunkBatch(**chunk_data)

                # Verify chunk structure (embeddings are added during pipeline execution)
                for chunk in chunk_batch.chunks:
                    assert chunk.id is not None
                    assert chunk.text is not None
                    assert chunk.doc_id is not None
                    assert isinstance(chunk.index, int)
                    assert isinstance(chunk.metadata, dict)

        # Verify that embeddings were actually generated by checking vector store
        # (The embeddings are added in-memory and stored to ChromaDB, not back to JSON files)
        vector_store = VectorStoreService()
        total_chunks_in_db = vector_store.count()
        # Since we can't easily isolate test chunks from existing ones, just verify the pipeline ran
        assert result["metrics"]["vectors_stored"] >= 1

    @pytest.mark.asyncio
    async def test_embedding_service_local_model(self):
        """Test that the embedding service uses the local model correctly."""
        embedder = EmbedderService()

        # Test single text embedding
        test_text = "This is a test sentence for embedding."
        embedding = embedder.embed_text(test_text)

        assert isinstance(embedding, list)
        assert len(embedding) == 384  # MiniLM-L6-v2 dimension
        assert all(isinstance(x, float) for x in embedding)

        # Test batch embedding
        test_texts = [
            "First test sentence.",
            "Second test sentence.",
            "Third test sentence with different content."
        ]
        embeddings = embedder.embed_batch(test_texts)

        assert isinstance(embeddings, list)
        assert len(embeddings) == 3
        for emb in embeddings:
            assert len(emb) == 384
            assert all(isinstance(x, float) for x in emb)

    def test_vector_store_operations(self):
        """Test vector store operations with embedded chunks."""
        # Create a temporary ChromaDB instance for testing
        temp_db_path = self.temp_dir / "test_chroma"
        vector_store = VectorStoreService(persist_path=str(temp_db_path))

        # Create test chunks with embeddings
        test_chunks = [
            Chunk(
                id="test_chunk_1",
                doc_id="test_doc_1",
                text="This is the first test chunk.",
                index=0,
                metadata={"source": "test.html", "job_id": "test_job"},
                embedding=[0.1] * 384  # Mock embedding
            ),
            Chunk(
                id="test_chunk_2",
                doc_id="test_doc_1",
                text="This is the second test chunk.",
                index=1,
                metadata={"source": "test.html", "job_id": "test_job"},
                embedding=[0.2] * 384  # Mock embedding
            )
        ]

        # Test adding chunks
        initial_count = vector_store.count()
        vector_store.add_chunks(test_chunks)
        final_count = vector_store.count()

        assert final_count >= initial_count + len(test_chunks)

        # Test querying
        query_embedding = [0.15] * 384  # Mock query embedding
        results = vector_store.query(query_embedding, n_results=5)

        assert results is not None
        assert "documents" in results
        assert "metadatas" in results
        assert len(results["documents"]) > 0

        # Clean up ChromaDB resources
        del vector_store

    @pytest.mark.asyncio
    async def test_pipeline_error_handling(self):
        """Test pipeline error handling for invalid files."""
        pipeline = PipelineManager()

        # Test with non-existent file (pipeline handles gracefully)
        result = await pipeline.run_file_pipeline(["/non/existent/file.html"])

        # Pipeline returns success even with no results, but metrics show the issue
        assert result["status"] == "success"
        assert result["metrics"]["files_submitted"] == 1
        assert result["metrics"]["parsed_successfully"] == 0  # No files parsed
        assert result["metrics"]["chunked_successfully"] == 0  # No chunks created
        assert result["metrics"]["vectors_stored"] == 0   # Nothing stored

        # Test with empty file list
        result = await pipeline.run_file_pipeline([])

        assert result["status"] == "success"
        assert result["metrics"]["files_submitted"] == 0

    def test_chunk_model_validation(self):
        """Test Chunk model validation and embedding field."""
        # Test chunk without embedding
        chunk = Chunk(
            id="test_chunk",
            doc_id="test_doc",
            text="Test chunk text",
            index=0,
            metadata={"source": "test"}
        )
        assert chunk.embedding is None

        # Test chunk with embedding
        embedding = [0.1, 0.2, 0.3] * 128  # 384 dimensions
        chunk_with_embedding = Chunk(
            id="test_chunk_embed",
            doc_id="test_doc",
            text="Test chunk with embedding",
            index=0,
            metadata={"source": "test"},
            embedding=embedding
        )
        assert chunk_with_embedding.embedding == embedding
        assert len(chunk_with_embedding.embedding) == 384

    @pytest.mark.asyncio
    async def test_pipeline_metrics_accuracy(self):
        """Test that pipeline metrics accurately reflect processing results."""
        # Create multiple test files
        test_files = []
        for i in range(3):
            test_file = self.test_data_dir / f"test_doc_{i}.html"
            test_file.write_text(f"""
            <!DOCTYPE html>
            <html>
            <head><title>Test Document {i}</title></head>
            <body>
                <h1>Test Document {i}</h1>
                <p>This is test content for document {i}.</p>
                <p>Additional content to ensure chunking occurs.</p>
            </body>
            </html>
            """)
            test_files.append(str(test_file))

        pipeline = PipelineManager()
        result = await pipeline.run_file_pipeline(test_files)

        # Verify metrics
        assert result["status"] == "success"
        assert result["metrics"]["files_submitted"] == 3
        assert result["metrics"]["parsed_successfully"] >= 1  # At least some should parse
        assert result["metrics"]["chunked_successfully"] >= 1  # At least some should chunk
        assert result["metrics"]["vectors_stored"] >= 1   # At least some should store

        # Verify artifacts
        assert len(result["artifacts"]["raw_files"]) == 3
        assert len(result["artifacts"]["parsed_files"]) >= 1
        assert len(result["artifacts"]["chunk_files"]) >= 1
